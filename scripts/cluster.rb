#!/usr/bin/env ruby
# scripts/cluster.rb
# Simple TF-IDF + KMeans clustering for microstreams (no external gems)
# Usage: ruby scripts/cluster.rb --k 8

require 'json'
require 'optparse'
require 'fileutils'

# ---- options
options = { k: 8, max_iter: 30 }
OptionParser.new do |opts|
  opts.banner = "Usage: cluster.rb [options]"
  opts.on("--k K", Integer, "Number of clusters (default 8)") { |v| options[:k] = v }
  opts.on("--max-iter N", Integer, "KMeans max iterations") { |v| options[:max_iter] = v }
end.parse!

# ---- utilities
STOPWORDS = %w[
  a an the and or of to in for on with is it this that by from as at be has have was were are
  i you he she they we my your our their its do does did so but not no yes can could should would
  our mine me him her them who what when where why how
]

def normalize(text)
  t = text.downcase
  t = t.gsub(/---\n.*?---\n/m, ' ') # strip YAML front matter if present
  t = t.gsub(/[^a-z0-9\s]/i, ' ')
  tokens = t.split(/\s+/).map(&:strip).reject(&:empty?)
  tokens.reject! { |w| STOPWORDS.include?(w) }
  tokens
end

def top_terms_from_corpus(corpus_tokens, top_n=200)
  freq = Hash.new(0)
  corpus_tokens.flatten.each { |t| freq[t] += 1 }
  freq.sort_by { |k,v| -v }[0, top_n].map(&:first)
end

def build_tf_vectors(docs_tokens, vocabulary)
  vectors = docs_tokens.map do |tokens|
    vec = Array.new(vocabulary.length, 0.0)
    counts = Hash.new(0)
    tokens.each { |t| counts[t] += 1 }
    vocabulary.each_with_index { |term, i| vec[i] = counts[term].to_f }
    vec
  end
  vectors
end

def idf_scale(vectors)
  n = vectors.length.to_f
  df = Array.new(vectors[0].length, 0)
  vectors.each do |v|
    v.each_with_index { |val,i| df[i] += 1 if val > 0 }
  end
  idf = df.map { |d| Math.log((n + 1) / (d + 1)) + 1.0 }
  vectors.map do |v|
    v.each_with_index.map { |val,i| val * idf[i] }
  end
end

def norm(v)
  Math.sqrt(v.map { |x| x*x }.inject(0.0, &:+))
end

def cosine(a,b)
  na = norm(a); nb = norm(b)
  return 0.0 if na == 0 || nb == 0
  dot = a.each_with_index.map { |x,i| x*b[i] }.inject(0.0, &:+)
  dot / (na*nb)
end

def add_vectors(a,b)
  a.each_with_index.map { |v,i| v + b[i] }
end

def scale_vector(a, scalar)
  a.map { |v| v * scalar }
end

# ---- read docs
files = Dir.glob("microstreams/*.md").sort
if files.empty?
  puts "No microstream files found in microstreams/  — exiting."
  exit 0
end

docs = files.map do |f|
  text = File.read(f)
  # naive front matter strip
  text = text.sub(/\A---.*?---\s*/m, '')
  { path: f, content: text }
end

docs_tokens = docs.map { |d| normalize(d[:content]) }
vocab = top_terms_from_corpus(docs_tokens, 300)
tf_vectors = build_tf_vectors(docs_tokens, vocab)
tfidf_vectors = idf_scale(tf_vectors)

# ---- KMeans
k = options[:k].to_i
k = [1, k, docs.length].sort[1]  # clamp
puts "Clustering #{docs.length} docs into k=#{k}"

# initialize centroids by picking k distinct docs
seed_idxs = (0...docs.length).to_a.sample(k)
centroids = seed_idxs.map { |i| tfidf_vectors[i].dup }

max_iter = options[:max_iter]
assignments = Array.new(docs.length, -1)

max_iter.times do |iter|
  changed = false
  # assign
  docs.each_with_index do |doc,i|
    best_j = nil
    best_sim = -1.0
    centroids.each_with_index do |c,j|
      sim = cosine(tfidf_vectors[i], c)
      if sim > best_sim
        best_sim = sim
        best_j = j
      end
    end
    if assignments[i] != best_j
      changed = true
      assignments[i] = best_j
    end
  end

  # recompute centroids
  new_centroids = Array.new(k) { Array.new(vocab.length, 0.0) }
  counts = Array.new(k, 0)
  assignments.each_with_index do |cluster,i|
    new_centroids[cluster] = add_vectors(new_centroids[cluster], tfidf_vectors[i])
    counts[cluster] += 1
  end
  new_centroids.each_with_index do |vec, j|
    if counts[j] > 0
      centroids[j] = scale_vector(vec, 1.0 / counts[j])
    else
      # re-seed empty centroid
      centroids[j] = tfidf_vectors.sample.dup
    end
  end

  puts "iter #{iter+1}: changed=#{changed}"
  break unless changed
end

# ---- produce cluster outputs
FileUtils.mkdir_p("clusters")

clusters = (0...k).map { |i| { docs: [], top_terms: [] } }

assignments.each_with_index do |cluster,i|
  clusters[cluster][:docs] << { path: docs[i][:path], snippet: docs[i][:content].strip.split("\n").map(&:strip).reject(&:empty?)[0,3].join(" ") }
end

# compute top terms per cluster
clusters.each_with_index do |cl, idx|
  term_scores = Hash.new(0.0)
  cl[:docs].each do |d|
    tokens = normalize(File.read(d[:path]))
    tokens.each { |t| term_scores[t] += 1 }
  end
  top = term_scores.sort_by { |k,v| -v }[0,20].map(&:first)
  cl[:top_terms] = top
  # write cluster file
  cluster_md = <<~MD
  ---
  layout: default
  title: Cluster #{idx+1}
  description: Autogenerated cluster (C-Engine) - #{cl[:top_terms][0,6].join(', ')}
  ---

  # Cluster #{idx+1}

  **Top terms:** #{cl[:top_terms][0,12].join(', ')}

  ## Documents (#{cl[:docs].length})

  #{cl[:docs].map.with_index { |d,i| "- [#{File.basename(d[:path])}](../#{d[:path]}) — #{d[:snippet]}" }.join("\n")}

  MD

  File.write("clusters/cluster-#{format('%02d', idx+1)}.md", cluster_md)
  File.write("clusters/cluster-#{format('%02d', idx+1)}.json", JSON.pretty_generate({
    id: idx+1,
    title: "Cluster #{idx+1}",
    top_terms: cl[:top_terms],
    docs: cl[:docs].map { |d| { path: d[:path], snippet: d[:snippet] } }
  }))
end

# write index map
map = {
  generated_at: Time.now.utc.iso8601,
  k: k,
  clusters: clusters.each_with_index.map { |cl, i|
    {
      id: i+1,
      top_terms: cl[:top_terms][0,12],
      doc_count: cl[:docs].length,
      filename: "clusters/cluster-#{format('%02d', i+1)}.md",
      json: "clusters/cluster-#{format('%02d', i+1)}.json"
    }
  }
}
File.write("clusters.json", JSON.pretty_generate(map))
puts "Wrote #{k} clusters -> clusters/ and clusters.json"
